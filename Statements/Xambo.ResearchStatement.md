# Research statement

## Vision & Mission
 
 I envision developing my research towards **sociocultural sound & music computing** to contribute to creating a more diverse and democratic community in the sound and music computing field. My mission is to (1) bring more democratic, diverse and playful experiences for creating and learning music computing using novel technologies (e.g. collaborations with museums, hack spaces), and (2) raise awareness of existing sociocultural-related issues from a sound and music computing perspective (e.g. [computational anthropology](https://www.technologyreview.com/s/528216/the-emerging-science-of-computational-anthropology/), data visualization). 
 
## Foci of research
 
 In particular, my interest focuses on finding synergies between social sciences and music computing research, and connect my two backgrounds in engineering and artistic digital humanities. Social science research provides theoretical frameworks for understanding social structures and processes. Sound and music computing research provides computational and engineering tools for understanding sound and music related phenomena. The discipline of HCI bridges the gap between social sciences and engineering by considering the human factors involved during the interactions with computers. My research is in alignment with HCI focusing on supporting sound and music computing within social applications. This research can be conducted in the field (*in the wild*), or in the lab. Data analysis includes qualitative as well as quantitative research methods. The outcomes include algorithms and systems that support new practices in social sound & music computing, research methods for understanding these social interactions, and experiences that embrace these social interactions.
 
My research has three foci:

1. Development of tools and experiences for engaging non-musicians, especially girls, into music technology.
2. Development of social projects and computing tools to raise awareness on sound-related issues. 
3. Development of real-time interactive systems and creative algorithms for supporting collaboration and social practices in education and performance.

### Development of tools and experiences for engaging non-musicians, especially girls, into music technology

This includes the development of STEAM educational tools that help students to engage with physics and maths through music technology. It also includes the development of tools that have a low-entry access to music making. For example, see my PhD work on tabletops and ambisonics as a collaborative tool for sound design and performance with everyday sounds. It also includes the organization and dissemination of workshops in alignment with [Yorkshire sound women network](https://yorkshiresoundwomen.wordpress.com/) workshops for girls in the UK or Hackathons in alignment with the inclusive [Monthly Music Hackathon NYC](monthlymusichackathon.org). It also includes the collaboration with existing high school programs, such as the Magnet Program; as well as collaborations with museums and STEAM programs. This branch is a follow-up of my own thesis work at the Open University, as well as current work with the projects EarSketch and TuneTable at Georgia Tech. 

* Xambó, A., Drozda, B., Weisling, A., Magerko, B., Huet, M., Gasque, T., Freeman, J. (submitted) Experience and ownership with a tangible computational music installation for informal learning.
* Xambó, A., Lerch, A., Freeman, J. (2016). Learning to code through MIR. In *Extended abstracts for the Late-Breaking Demo Session of the 17th International Society for Music Information Retrieval Conference (ISMIR 2016)*. New York.
* Xambó, A. (2015). *Tabletop Tangible Interfaces for Music Performance: Design and Evaluation*. Thesis. The Open University.
* Freeman, J., Magerko, B., Edwards, D., Moore, R., McKlin, T., Xambó, A. (2015). *EarSketch: a STEAM approach to broadening participation in computer science principles*. In Proceedings of the IEEE Research in Equity and Sustained Participation in Engineering, Computing, and Technology (RESPECT '15). Charlotte, NC. pp. 109-110.
* Xambó, A., Roma, G., Laney, R., Dobbyn, C. and Jordà, S. (2014). SoundXY4: supporting tabletop collaboration and awareness with ambisonics spatialisation. In *Proceedings of the International Conference on New Interfaces for Musical Expression 2014 (NIME '14)*. London. pp. 249–252.

## Development of social projects and computing tools to raise awareness on sound-related issues

This research is in alignment with the [environmental justice movement](http://serve-learn-sustain.gatech.edu/environmental-justice-series) in Georgia Tech related to sound-related issues e.g. noise pollution. 
Actions include participatory design and research e.g. working with low-income communities, and raising awareness of existing problems with computational tools, for example, visualization of world-wide data, or making urban interventions, such as the creation of situated sound installations. This branch also aligns with research in social computing developed at the School of Interactive Computing at Georgia Tech and research in visualization developed at the College of Computing. 

* Started conversations with Jennifer Hirsch (Director of Center for Serve-Learn-Sustain, Georgia Institute of Technology) for the organization in Spring 2017 of an event around noise pollution within the mark of the environmental justice series.
* I have been always interesed in data visualization as an aiding tool within the sound and music computing domain, see:
	* Xambó, A., Lerch, A., Freeman, J. (2016). Learning to code through MIR. In *Extended abstracts for the Late-Breaking Demo Session of the 17th International Society for Music Information Retrieval Conference (ISMIR 2016)*. New York. 
	* Bogdanov, D., Haro, M., Fuhrmann, F., Xambó, A., Gómez, E. and Herrera, P. (2013). Semantic audio content-based music recommendation and visualization based on user preference examples. *Information Processing & Management*, 49(1), pp. 13-33.
	* Haro, M.; Xambó, A.; Fuhrmann, F.; Bogdanov, D.; Gómez, E. and Herrera, P. (2010). The Musical Avatar: a visualization of musical preferences by means of audio content description. In *Proceedings of the 5th Audio Mostly Conference (AM '10)*. Piteå, Sweden.
	* Roma, G. and Xambó, A. (2008). A tabletop waveform editor for live performance. In *Proceedings of the International Conference on New Interfaces for Musical Expression (NIME '08)*. Genoa, Italy.

## Development of real-time interactive systems and creative algorithms for supporting collaboration and social practices in education and performance

This research includes the development of collaborative live coding interfaces, tangible interfaces, and network music platforms. 
This branch is also a follow-up of my own thesis work, the experience from my own practice as well as from EarSketch and its live coding capabilities. The evaluation of these systems would follow up my previous research on studying music improvisation with novel technologies as understanding open forms in the wild, borrowing methodological tools from social sciences. 

* Xambó, A., Freeman, J., Magerko, B., Shah, P. (2016). Challenges and new directions for collaborative live coding in the classroom. In *International Conference of Live Interfaces (ICLI 2016)*. Brighton, UK.
* Xambó, A., Hornecker, E., Marshall, P., Jordà, S., Dobbyn, C. and Laney, R. (2016). Exploring social interaction with a tangible music interface. *Interacting with Computers*.
* Xambó, A. (2015). *Tabletop Tangible Interfaces for Music Performance: Design and Evaluation*. Thesis. The Open University.
* Xambó, A., Hornecker, E., Marshall, P., Jordà, S., Dobbyn, C. and Laney, R. (2013). *Let's jam the Reactable: peer learning during musical improvisation with a tabletop tangible interface*. ACM Transactions on Computer-Human Interaction (TOCHI), 20(6), pp. 36:1–36:34.
 
Atlanta, October 16, 2016

Anna Xambó